{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"hlUbQMxUTdec"}},{"cell_type":"code","source":["import json\n","import re\n","import random\n","import time\n","import openai\n","import os"],"metadata":{"id":"1sK2v2A0QJYn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load data"],"metadata":{"id":"zXWxM1jeT1oa"}},{"cell_type":"code","source":["# Create a lookup for the pdf parse based on paper ID\n","corpus = {}\n","with open(f'../data/scifact/corpus.jsonl') as f_pdf:\n","    for line in f_pdf:\n","        pdf_parse_dict = json.loads(line)\n","        corpus[pdf_parse_dict['doc_id']] = pdf_parse_dict\n","print(\"Corpus parsed.\")"],"metadata":{"id":"uUGPWPheTodF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["claim_id_to_claim = []\n","# Retrieve all claims for the train set\n","with open(f'../data/scifact/claims_train.jsonl') as f_pdf:\n","    for line in f_pdf:\n","        claim_dict = json.loads(line)\n","        if (claim_dict['evidence']):\n","          claim_id_to_claim.append(claim_dict)\n","# Retireve all claims from the dev set\n","with open(f'../data/scifact/claims_dev.jsonl') as f_pdf:\n","    for line in f_pdf:\n","        claim_dict = json.loads(line)\n","        if (claim_dict['evidence']):\n","          claim_id_to_claim.append(claim_dict)\n","\n","print(\"Claims parsed.\")"],"metadata":{"id":"me-yGAQtT7Vn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a lookup for the pdf parse based on paper ID\n","citances = []\n","with open(f'../data/scifact/claims_with_citances.jsonl') as f_pdf:\n","    for line in f_pdf:\n","        citance_dict = json.loads(line)\n","        citances.append((re.sub(r' \\[\\d+\\]', '', citance_dict['citance']), citance_dict['claims']))\n","        # citances.append((citance_dict['citance'], citance_dict['claims']))\n","print(\"Citances parsed.\")\n","print('Number of Citances:', len(citances))\n","\n","claims_and_evidence = {}\n","for i in claim_id_to_claim:\n","    i['normalized_claim'] = re.sub(r'[.!?,\\s]', '', i['claim'].lower())\n","    claims_and_evidence[i['normalized_claim']] = i['evidence']\n","\n","citance_to_id_dict = {}\n","with open(f'../data/scifact/claims_with_citances.jsonl') as f_pdf:\n","    for line in f_pdf:\n","        citance_dict = json.loads(line)\n","        citance_to_id_dict[re.sub(r' \\[\\d+\\]', '', citance_dict['citance'])] = citance_dict['s2orc_id']"],"metadata":{"id":"cvEZkqn4T-6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Negation Generation and Validation\n"],"metadata":{"id":"j_lm_5X8qmwS"}},{"cell_type":"code","source":["# Load .env file with your API key\n","load_dotenv()\n","openai.api_key = os.getenv('OPENAI_API_KEY')"],"metadata":{"id":"ycUjmuyTlg4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["negations = {}\n","failures = 0\n","successes = 0\n","prompt = \"Please negate this sentence by changing as few words as possible in the original sentence: \"\n","# Loop through all citances\n","for citance in citances:\n","  time.sleep(5.5)\n","  citance = citance[0]\n","  # Use current citance as a variable in the standard prompt\n","  response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt + citance, temperature=0, max_tokens=256) ######### THIS MODEL HAS BEEN DEPRECATED\n","  response = response.choices[0].text[2:]\n","\n","  # Checking generated negation length within 20% of original\n","  rlen = len(response.split())\n","  clen = len(citance.split())\n","  if (rlen > clen * 1.2 or rlen < clen * 0.8):\n","    failures += 1\n","    continue\n","  else:\n","    negations[citance] = response\n","    successes += 1\n","  negations[citance] = response\n","print(\"Successes:\", successes)\n","print(\"Failures: \", failures)"],"metadata":{"id":"28WDxEO4qpiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["obj = json.dumps(negations, indent=4)\n","with open('../data/negations/negations.json', 'w') as f:\n","  f.write(obj)"],"metadata":{"id":"lZxVkEF9u9jl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"Given two sentences, please evaluate the extent to which the second sentence is a negation of the first. Please provide a confidence score in the domain [0, 100], where a score of 0 means identical meaning and a score of 100 means a perfect negation. \\nFirst sentence: {} \\nSecond sentence {}\"\n","results = []\n","failures = 0\n","for key in negations:\n","  time.sleep(1)\n","  prompt = query.format(key, negations[key])\n","  message = [{\"role\": \"user\", \"content\": prompt}]\n","  try:\n","    response = openai.ChatCompletion.create(model=\"gpt-4\", messages=message, temperature=0.2)\n","  except:\n","    failures += 1\n","    continue\n","  score = response.choices[0].message.content\n","  results.append((key, negations[key], score))\n","\n","print(\"Failures: \", failures)"],"metadata":{"id":"-adHWdh3tvH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["negation_score_dict = {}\n","for item in results:\n","  negation_score_dict[item[0]] = [item[1], item[2]]\n","obj = json.dumps(negation_score_dict, indent=4)\n","\n","# Write to file\n","with open('../data/negations/negations_with_scores.json', 'w') as f:\n","  f.write(obj)"],"metadata":{"id":"_-K4yrgdt4dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Citance to Claim to Evidence Matching"],"metadata":{"id":"mfS814F5dlNe"}},{"cell_type":"code","source":["with open(f'../data/negations/negations.json') as f:\n","    negations = json.load(f)"],"metadata":{"id":"TwOx0qRna6CX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup of SUPPORTS and CONTRADICTS\n","data = {}\n","dataset_corpus = {}\n","lines = []\n","failed_keys = []\n","errors = 0\n","int_id = 0\n","counter = 0\n","citance_set = set()\n","# Loop through citances dict\n","for citance in citances:\n","    # For each citance, loop through all claims\n","    for claim in citance[1]:\n","        claim['normalized'] = re.sub(r'[.!?,\\s]', '', claim['text'].lower())\n","        # Skip if there is no matching claim\n","        if claim['normalized'] not in claims_and_evidence:\n","            failed_keys.append(claim['normalized'])\n","            continue\n","\n","        support = {}\n","        contradict = {}\n","        # For each associated claim, loop through all associated evidence\n","        for doc_id in claims_and_evidence[claim['normalized']]:\n","\n","            for sentences in claims_and_evidence[claim['normalized']][doc_id]:\n","                # Negations of the citance\n","                if sentences['label'] == 'SUPPORT': # Only take SUPPORT labels\n","                    # Actual citance\n","                    support['claim'] = citance[0]\n","                    support['id'] = int_id\n","                    int_id += 1\n","\n","                    support['evidence'] = {int(doc_id): [{\n","                          'label': 'SUPPORT'\n","                          }]\n","                          }\n","                    support['citance_id'] = citance_to_id_dict[citance[0]]\n","                    support['doc_ids'] = [int(doc_id)]\n","\n","                    # Negated citance\n","                    try:\n","                        contradict['claim'] = negations[citance[0]]\n","                    except Exception as e: # key error\n","                        errors += 1\n","                        continue\n","                    contradict['id'] = int_id\n","                    int_id += 1\n","\n","                    contradict['evidence'] = {int(doc_id): [{\n","                          'label': 'CONTRADICT'\n","                          }]\n","                          }\n","                    contradict['citance_id'] = citance_to_id_dict[citance[0]]\n","                    contradict['doc_ids'] = [int(doc_id)]\n","\n","                    data[citance[0]] = support # Create new data entry for SUPPORT and CONTRADICT\n","                    data[negations[citance[0]]] = contradict\n","\n","\n","                    dataset_corpus[int(doc_id)] = { # Update corpus with necessary docs\n","                        'title': corpus[int(doc_id)]['abstract'][0],\n","                        'abstract': corpus[int(doc_id)]['abstract'],\n","                        'doc_id': int(doc_id)\n","                    }\n","print(errors)\n","print(len(failed_keys))\n","print(len(data.keys()))"],"metadata":{"id":"_JwFaTztb57b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# NEI Generation"],"metadata":{"id":"25WCV7hwX_Tw"}},{"cell_type":"code","source":["claim_id_to_claim = []\n","with open(f'/data/scifact/claims_train.jsonl') as f_pdf:\n","    for line in f_pdf:\n","        claim_dict = json.loads(line)\n","        nei_ids = []\n","        for id in claim_dict['cited_doc_ids']:\n","            if str(id) not in claim_dict['evidence']:\n","                nei_ids.append(id)\n","        if not nei_ids:\n","          continue\n","        claim_dict['cited_doc_ids'] = nei_ids\n","        claim_dict['evidence'] = {}\n","        claim_id_to_claim.append(claim_dict)\n","with open(f'/data/scifact/claims_dev.jsonl') as f_pdf:\n","    for line in f_pdf:\n","        claim_dict = json.loads(line)\n","        nei_ids = []\n","        for id in claim_dict['cited_doc_ids']:\n","            if str(id) not in claim_dict['evidence']:\n","                nei_ids.append(id)\n","        if not nei_ids:\n","          continue\n","        claim_dict['cited_doc_ids'] = nei_ids\n","        claim_dict['evidence'] = {}\n","        claim_id_to_claim.append(claim_dict)\n","print(\"Claims parsed.\")\n","\n","claims_and_doc_ids = {}\n","for i in claim_id_to_claim:\n","    i['normalized_claim'] = re.sub(r'[.!?,\\s]', '', i['claim'].lower())\n","    claims_and_doc_ids[i['normalized_claim']] = i['cited_doc_ids']"],"metadata":{"id":"sbD7vbkAc59O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup of NEI\n","citances_to_id = {}\n","results = []\n","failed_keys = []\n","errors = 0\n","# Loop through citances dict\n","for citance in citances:\n","    # For each citance, loop through all claims\n","    for claim in citance[1]:\n","\n","        claim['normalized'] = re.sub(r'[.!?,\\s]', '', claim['text'].lower())\n","        nei = {}\n","        if claim['normalized'] in claims_and_doc_ids:\n","            ids = []\n","            for doc_id in claims_and_doc_ids[claim['normalized']]: # Update corpus with necessary docs\n","                if id not in ids:\n","                    ids.append(int(doc_id))\n","                    dataset_corpus[int(doc_id)] = {\n","                                'title': corpus[int(doc_id)]['abstract'][0],\n","                                'abstract': corpus[int(doc_id)]['abstract'],\n","                                'doc_id': int(doc_id)\n","                            }\n","            nei['claim'] = citance[0]\n","            nei['id'] = int_id\n","            nei['evidence'] = {}\n","            nei['citance_id'] = citance_to_id_dict[citance[0]]\n","            nei['doc_ids'] = ids\n","            if citance[0] not in data: # Add new entry for NEI\n","                data[citance[0]] = nei\n","                num += len(ids)\n","                int_id += 1\n","            else: # Update existing entry for NEI\n","              for id in ids:\n","                if id not in outer[citance[0]]['doc_ids']:\n","                  data[citance[0]]['doc_ids'].append(id)\n","\n","\n","print(errors)\n","print(len(failed_keys))"],"metadata":{"id":"AhixW4QIYKJ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save dataset"],"metadata":{"id":"PN41cWviCwkn"}},{"cell_type":"code","source":["random.seed(5)\n","m = list(data.values())\n","random.shuffle(m)\n","split_1 = int(0.7 * len(m))\n","split_2 = int(0.85 * len(m))\n","train = m[:split_1]\n","dev = m[split_1:split_2]\n","test = m[split_2:]\n","\n","with open('../data/scitance/train.jsonl', \"w\") as f:\n","    for item in train:\n","        json_item = json.dumps(item)\n","        f.write(json_item + \"\\n\")\n","\n","with open('../data/scitance/dev.jsonl', \"w\") as f:\n","    for item in dev:\n","        json_item = json.dumps(item)\n","        f.write(json_item + \"\\n\")\n","\n","with open('../data/scitance/test.jsonl', \"w\") as f:\n","    for item in test:\n","        json_item = json.dumps(item)\n","        f.write(json_item + \"\\n\")\n","\n","with open('../data/scitance/corpus.jsonl', \"w\") as f:\n","    for item in list(dataset_corpus.values()):\n","        json_item = json.dumps(item)\n","        f.write(json_item + \"\\n\")"],"metadata":{"id":"WAf3CvfgsKnQ"},"execution_count":null,"outputs":[]}]}